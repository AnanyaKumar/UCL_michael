name: simsiam-c10-experiment-resnet18
dataset: 
  name: fmow
  image_size: 32
  num_workers: 4

model: 
  name: simsiam
  backbone: densenet121
  cl_model: finetune
  proj_layers: 2
  buffer_size: 256

train:
  disable_logging: False
  cl_default: True
  optimizer: 
    name: adam
    weight_decay: 0.0
    momentum: 0.0
  save_best: False
  naive: False
  warmup_epochs: 10
  warmup_lr: 0
  base_lr: 0.03
  final_lr: 0
  ft_lr: 0.0008
  lp_lr: 0.0008
  grad_thresh: -1.
  grad_by_layer: False
  freeze_include_head: False
  num_lp_epochs: 25
  num_epochs: 50 # this parameter influence the lr decay
  all_tasks_num_epochs: 0
  stop_at_epoch: 50 # has to be smaller than num_epochs
  ft_first: False
  train_first: False
  reset_lp_lr: True
  lp_epoch_frac: 0.5
  batch_size: 32
  knn_monitor: True # knn monitor will take more time
  knn_interval: 1
  probe_monitor: True
  probe_interval: 1
  knn_k: 200
  alpha: 0.4
eval: # linear evaluation, False will turn off automatic evaluation after training
  optimizer: 
    name: sgd
    weight_decay: 0
    momentum: 0.9
  warmup_lr: 0
  warmup_epochs: 0
  base_lr: 30
  final_lr: 0
  batch_size: 256
  num_epochs: 100

logger:
  csv_log: True
  tensorboard: True
  matplotlib: True

seed: null # None type for yaml file
# two things might lead to stochastic behavior other than seed:
# worker_init_fn from dataloader and torch.nn.functional.interpolate 
# (keep this in mind if you want to achieve 100% deterministic)




